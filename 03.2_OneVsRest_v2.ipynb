{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, hamming_loss, jaccard_score, accuracy_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'title', 'tags', 'score', 'answer_count', 'sentence_bow',\n",
      "       'sentence_bow_lem', 'sentence_dl'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('stackoverflow_questions_cleaned_train.csv')\n",
    "df_test = pd.read_csv('stackoverflow_questions_cleaned_test.csv')\n",
    "\n",
    "# Afficher les colonnes disponibles\n",
    "print(df_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using splitted train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1408 entries, 0 to 2008\n",
      "Data columns (total 9 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   date              1408 non-null   object\n",
      " 1   title             1408 non-null   object\n",
      " 2   tags              1408 non-null   object\n",
      " 3   score             1408 non-null   int64 \n",
      " 4   answer_count      1408 non-null   int64 \n",
      " 5   sentence_bow      1408 non-null   object\n",
      " 6   sentence_bow_lem  1408 non-null   object\n",
      " 7   sentence_dl       1408 non-null   object\n",
      " 8   filtered_tags     1408 non-null   object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 110.0+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1408 entries, 0 to 2008\n",
      "Data columns (total 9 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   date              1408 non-null   object\n",
      " 1   title             1408 non-null   object\n",
      " 2   tags              1408 non-null   object\n",
      " 3   score             1408 non-null   int64 \n",
      " 4   answer_count      1408 non-null   int64 \n",
      " 5   sentence_bow      1408 non-null   object\n",
      " 6   sentence_bow_lem  1408 non-null   object\n",
      " 7   sentence_dl       1408 non-null   object\n",
      " 8   filtered_tags     1408 non-null   object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 110.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "number_of_tags = 30\n",
    "\n",
    "# Créer une liste de tous les tags\n",
    "all_tags = [tag for tags in df_train['tags'].apply(eval) for tag in tags]  # Utiliser eval pour convertir les chaînes de listes en listes\n",
    "\n",
    "# Limiter les tags aux plus fréquents\n",
    "top_tags = [tag for tag, count in Counter(all_tags).most_common(number_of_tags)]\n",
    "\n",
    "# Filtrer les tags pour ne garder que les top \n",
    "df_train['filtered_tags'] = df_train['tags'].apply(lambda tags: [tag for tag in eval(tags) if tag in top_tags])\n",
    "# # Filtrer les tags pour ne garder que les top \n",
    "# df_test['filtered_tags'] = df_test['tags'].apply(lambda tags: [tag for tag in eval(tags) if tag in top_tags])\n",
    "\n",
    "print(df_test.info())\n",
    "\n",
    "# Supprimer les lignes sans tags pour df_train\n",
    "df_train = df_train[df_train['filtered_tags'].map(len) > 0]\n",
    "df_test = df_test[df_test['filtered_tags'].map(len) > 0]\n",
    "print(df_test.info())\n",
    "\n",
    "\n",
    "# Encoder les tags avec MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(classes=top_tags)\n",
    "y_train = mlb.fit_transform(df_train['filtered_tags'])\n",
    "y_test = mlb.fit_transform(df_test['filtered_tags'])\n",
    "\n",
    "tfidf_max_features = 2000\n",
    "# Vectorisation TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=tfidf_max_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour entraîner, évaluer et logguer le modèle\n",
    "def train_and_log_model(column_name, svd_components=300):\n",
    "    print(f\"Training model for column: {column_name}\")\n",
    "\n",
    "    X_tfidf_train = tfidf_vectorizer.fit_transform(df_train[column_name])\n",
    "    X_tfidf_test = tfidf_vectorizer.fit_transform(df_test[column_name])\n",
    "\n",
    "    # Réduction dimensionnelle avec SVD\n",
    "    svd = TruncatedSVD(n_components=svd_components)\n",
    "    X_train = svd.fit_transform(X_tfidf_train)\n",
    "    X_test = svd.transform(X_tfidf_test)\n",
    "\n",
    "    # Entraîner le modèle OneVsRestClassifier avec LogisticRegression\n",
    "    model = OneVsRestClassifier(LogisticRegression(max_iter=1000))\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Prédire sur l'ensemble de test\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculer et afficher les scores\n",
    "    f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "    hamming = hamming_loss(y_test, y_pred)\n",
    "    jaccard = jaccard_score(y_test, y_pred, average='samples')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Score F1 (micro) for {column_name}: {f1_micro}\")\n",
    "    print(f\"Score F1 (weighted) for {column_name}: {f1_weighted}\")\n",
    "    print(f\"Hamming Loss for {column_name}: {hamming}\")\n",
    "    print(f\"Jaccard Score for {column_name}: {jaccard}\")\n",
    "    print(f\"Accuracy for {column_name}: {accuracy}\")\n",
    "    print('-------------------------------------------------------------')\n",
    "\n",
    "    # Logger le modèle et les métriques sur MLflow\n",
    "    with mlflow.start_run(run_name=f\"{column_name}_model\") as run:\n",
    "    # Loguer les paramètres\n",
    "        mlflow.log_param(\"number_of_tags\", number_of_tags)\n",
    "        mlflow.log_param(\"max_features\", tfidf_max_features)\n",
    "        mlflow.log_param(\"n_components\", svd_components)\n",
    "        mlflow.log_param(\"test_size\", 0.2)\n",
    "        mlflow.log_param(\"model\", \"LogisticRegression - TF-IDF + SVD\")\n",
    "        \n",
    "        mlflow.log_param(\"column\", column_name)\n",
    "        mlflow.log_metric(\"f1_score_micro\", f1_micro)\n",
    "        mlflow.log_metric(\"f1_score_weighted\", f1_weighted)\n",
    "        mlflow.log_metric(\"hamming_loss\", hamming)\n",
    "        mlflow.log_metric(\"jaccard_score\", jaccard)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "        # Logger les modèles\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        mlflow.sklearn.log_model(tfidf_vectorizer, \"tfidf_vectorizer\")\n",
    "        mlflow.sklearn.log_model(svd, \"svd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for column: title\n",
      "Score F1 (micro) for title: 0.0031965903036760787\n",
      "Score F1 (weighted) for title: 0.0031159511519576986\n",
      "Hamming Loss for title: 0.04429450757575758\n",
      "Jaccard Score for title: 0.0014204545454545455\n",
      "Accuracy for title: 0.0007102272727272727\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/17 11:28:24 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/07/17 11:28:26 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for column: sentence_bow\n",
      "Score F1 (micro) for sentence_bow: 0.004289544235924933\n",
      "Score F1 (weighted) for sentence_bow: 0.004280664206153051\n",
      "Hamming Loss for sentence_bow: 0.04396306818181818\n",
      "Jaccard Score for sentence_bow: 0.0024857954545454545\n",
      "Accuracy for sentence_bow: 0.002130681818181818\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/17 11:28:39 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/07/17 11:28:43 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for column: sentence_bow_lem\n",
      "Score F1 (micro) for sentence_bow_lem: 0.0021413276231263384\n",
      "Score F1 (weighted) for sentence_bow_lem: 0.0021111330280364866\n",
      "Hamming Loss for sentence_bow_lem: 0.04412878787878788\n",
      "Jaccard Score for sentence_bow_lem: 0.0014204545454545455\n",
      "Accuracy for sentence_bow_lem: 0.0014204545454545455\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/17 11:29:00 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/07/17 11:29:03 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    }
   ],
   "source": [
    "# Entraîner et logguer les modèles pour chaque colonne\n",
    "columns_to_train = ['title', 'sentence_bow', 'sentence_bow_lem']\n",
    "\n",
    "for column in columns_to_train:\n",
    "    train_and_log_model(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using full df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le DataFrame nettoyé\n",
    "df = pd.read_csv('stackoverflow_questions_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_tags = 30\n",
    "\n",
    "# Créer une liste de tous les tags\n",
    "all_tags = [tag for tags in df['tags'].apply(eval) for tag in tags]  # Utiliser eval pour convertir les chaînes de listes en listes\n",
    "\n",
    "# Limiter les tags aux plus fréquents\n",
    "top_tags = [tag for tag, count in Counter(all_tags).most_common(number_of_tags)]\n",
    "\n",
    "# Filtrer les tags pour ne garder que les top \n",
    "df['filtered_tags'] = df['tags'].apply(lambda tags: [tag for tag in eval(tags) if tag in top_tags])\n",
    "\n",
    "# Supprimer les lignes sans tags pour df_train\n",
    "df = df[df['filtered_tags'].map(len) > 0]\n",
    "\n",
    "# Encoder les tags avec MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(classes=top_tags)\n",
    "y = mlb.fit_transform(df['filtered_tags'])\n",
    "\n",
    "tfidf_max_features = 150\n",
    "# Vectorisation TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=tfidf_max_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour entraîner, évaluer et logguer le modèle\n",
    "def train_and_log_model(column_name, svd_components=100):\n",
    "    print(f\"Training model for column: {column_name}\")\n",
    "\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(df[column_name])\n",
    "\n",
    "    # Réduction dimensionnelle avec SVD\n",
    "    svd = TruncatedSVD(n_components=svd_components)\n",
    "    X_reduced = svd.fit_transform(X_tfidf)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Entraîner le modèle OneVsRestClassifier avec LogisticRegression\n",
    "    model = OneVsRestClassifier(LogisticRegression(max_iter=1000))\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Prédire sur l'ensemble de test\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculer et afficher les scores\n",
    "    f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "    hamming = hamming_loss(y_test, y_pred)\n",
    "    jaccard = jaccard_score(y_test, y_pred, average='samples')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Score F1 (micro) for {column_name}: {f1_micro}\")\n",
    "    print(f\"Score F1 (weighted) for {column_name}: {f1_weighted}\")\n",
    "    print(f\"Hamming Loss for {column_name}: {hamming}\")\n",
    "    print(f\"Jaccard Score for {column_name}: {jaccard}\")\n",
    "    print(f\"Accuracy for {column_name}: {accuracy}\")\n",
    "    print('-------------------------------------------------------------')\n",
    "\n",
    "    # Logger le modèle et les métriques sur MLflow\n",
    "    with mlflow.start_run(run_name=f\"{column_name}_model\") as run:\n",
    "    # Loguer les paramètres\n",
    "        mlflow.log_param(\"number_of_tags\", number_of_tags)\n",
    "        mlflow.log_param(\"max_features\", tfidf_max_features)\n",
    "        mlflow.log_param(\"n_components\", svd_components)\n",
    "        mlflow.log_param(\"test_size\", 0.2)\n",
    "        mlflow.log_param(\"model\", \"LogisticRegression - TF-IDF + SVD\")\n",
    "        \n",
    "        mlflow.log_param(\"column\", column_name)\n",
    "        mlflow.log_metric(\"f1_score_micro\", f1_micro)\n",
    "        mlflow.log_metric(\"f1_score_weighted\", f1_weighted)\n",
    "        mlflow.log_metric(\"hamming_loss\", hamming)\n",
    "        mlflow.log_metric(\"jaccard_score\", jaccard)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "        # Logger les modèles\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        mlflow.sklearn.log_model(tfidf_vectorizer, \"tfidf_vectorizer\")\n",
    "        mlflow.sklearn.log_model(svd, \"svd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for column: title\n",
      "Score F1 (micro) for title: 0.32220738762794837\n",
      "Score F1 (weighted) for title: 0.2882674419163026\n",
      "Hamming Loss for title: 0.035700890764181904\n",
      "Jaccard Score for title: 0.19503047351148617\n",
      "Accuracy for title: 0.16385372714486637\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/17 11:34:09 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/07/17 11:34:11 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for column: sentence_bow\n",
      "Score F1 (micro) for sentence_bow: 0.3405002193944713\n",
      "Score F1 (weighted) for sentence_bow: 0.3040585052380162\n",
      "Hamming Loss for sentence_bow: 0.035232067510548526\n",
      "Jaccard Score for sentence_bow: 0.20264885138302857\n",
      "Accuracy for sentence_bow: 0.1659634317862166\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/17 11:34:18 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/07/17 11:34:20 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for column: sentence_bow_lem\n",
      "Score F1 (micro) for sentence_bow_lem: 0.3244916003536693\n",
      "Score F1 (weighted) for sentence_bow_lem: 0.28704729588855626\n",
      "Hamming Loss for sentence_bow_lem: 0.03581809657759025\n",
      "Jaccard Score for sentence_bow_lem: 0.19157290201593996\n",
      "Accuracy for sentence_bow_lem: 0.15611814345991562\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/17 11:34:31 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/07/17 11:34:34 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    }
   ],
   "source": [
    "# Entraîner et logguer les modèles pour chaque colonne\n",
    "columns_to_train = ['title', 'sentence_bow', 'sentence_bow_lem']\n",
    "\n",
    "for column in columns_to_train:\n",
    "    train_and_log_model(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using full df and removing questions without tags only in train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le DataFrame nettoyé\n",
    "df = pd.read_csv('stackoverflow_questions_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_tags = 30\n",
    "\n",
    "# Créer une liste de tous les tags\n",
    "all_tags = [tag for tags in df['tags'].apply(eval) for tag in tags]  # Utiliser eval pour convertir les chaînes de listes en listes\n",
    "\n",
    "# Limiter les tags aux plus fréquents\n",
    "top_tags = [tag for tag, count in Counter(all_tags).most_common(number_of_tags)]\n",
    "\n",
    "# Filtrer les tags pour ne garder que les top \n",
    "df['filtered_tags'] = df['tags'].apply(lambda tags: [tag for tag in eval(tags) if tag in top_tags])\n",
    "\n",
    "# Séparer les questions avec et sans tags\n",
    "df_with_tags = df[df['filtered_tags'].map(len) > 0]\n",
    "df_without_tags = df[df['filtered_tags'].map(len) == 0]\n",
    "\n",
    "# Encoder les tags avec MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(classes=top_tags)\n",
    "y = mlb.fit_transform(df_with_tags['filtered_tags'])\n",
    "\n",
    "tfidf_max_features = 150\n",
    "# Vectorisation TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=tfidf_max_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour entraîner, évaluer et logguer le modèle\n",
    "def train_and_log_model(column_name, svd_components=100):\n",
    "    print(f\"Training model for column: {column_name}\")\n",
    "\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(df_with_tags[column_name])\n",
    "\n",
    "    # Réduction dimensionnelle avec SVD\n",
    "    svd = TruncatedSVD(n_components=svd_components)\n",
    "    X_reduced = svd.fit_transform(X_tfidf)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Ajouter les questions sans tags au jeu de test\n",
    "    X_test_no_tags = tfidf_vectorizer.transform(df_without_tags[column_name])\n",
    "    X_test_no_tags_reduced = svd.transform(X_test_no_tags)\n",
    "    y_test_no_tags = [[0]*len(top_tags)] * len(df_without_tags)\n",
    "\n",
    "    X_test = pd.concat([pd.DataFrame(X_test), pd.DataFrame(X_test_no_tags_reduced)], ignore_index=True)\n",
    "    y_test = pd.concat([pd.DataFrame(y_test), pd.DataFrame(y_test_no_tags)], ignore_index=True)\n",
    "\n",
    "    # Entraîner le modèle OneVsRestClassifier avec LogisticRegression\n",
    "    model = OneVsRestClassifier(LogisticRegression(max_iter=1000))\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Prédire sur l'ensemble de test\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculer et afficher les scores\n",
    "    f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "    hamming = hamming_loss(y_test, y_pred)\n",
    "    jaccard = jaccard_score(y_test, y_pred, average='samples')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Score F1 (micro) for {column_name}: {f1_micro}\")\n",
    "    print(f\"Score F1 (weighted) for {column_name}: {f1_weighted}\")\n",
    "    print(f\"Hamming Loss for {column_name}: {hamming}\")\n",
    "    print(f\"Jaccard Score for {column_name}: {jaccard}\")\n",
    "    print(f\"Accuracy for {column_name}: {accuracy}\")\n",
    "    print('-------------------------------------------------------------')\n",
    "\n",
    "    # Logger le modèle et les métriques sur MLflow\n",
    "    with mlflow.start_run(run_name=f\"{column_name}_model\") as run:\n",
    "        # Loguer les paramètres\n",
    "        mlflow.log_param(\"number_of_tags\", number_of_tags)\n",
    "        mlflow.log_param(\"max_features\", tfidf_max_features)\n",
    "        mlflow.log_param(\"n_components\", svd_components)\n",
    "        mlflow.log_param(\"test_size\", 0.2)\n",
    "        mlflow.log_param(\"model\", \"LogisticRegression - TF-IDF + SVD\")\n",
    "        \n",
    "        mlflow.log_param(\"column\", column_name)\n",
    "        mlflow.log_metric(\"f1_score_micro\", f1_micro)\n",
    "        mlflow.log_metric(\"f1_score_weighted\", f1_weighted)\n",
    "        mlflow.log_metric(\"hamming_loss\", hamming)\n",
    "        mlflow.log_metric(\"jaccard_score\", jaccard)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "        # Logger les modèles\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        mlflow.sklearn.log_model(tfidf_vectorizer, \"tfidf_vectorizer\")\n",
    "        mlflow.sklearn.log_model(svd, \"svd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for column: title\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonydavid/Documents/Etudes/alternance_ML_engineer/OpenClassrooms/projets/projet_5/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score F1 (micro) for title: 0.3004577611319184\n",
      "Score F1 (weighted) for title: 0.26985365036968845\n",
      "Hamming Loss for title: 0.012854630266880784\n",
      "Jaccard Score for title: 0.06350844995029441\n",
      "Accuracy for title: 0.6930488644184446\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/17 11:41:19 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/07/17 11:41:20 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for column: sentence_bow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonydavid/Documents/Etudes/alternance_ML_engineer/OpenClassrooms/projets/projet_5/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score F1 (micro) for sentence_bow: 0.31840796019900497\n",
      "Score F1 (weighted) for sentence_bow: 0.28283383289969255\n",
      "Hamming Loss for sentence_bow: 0.012571690754760266\n",
      "Jaccard Score for sentence_bow: 0.06509520532232163\n",
      "Accuracy for sentence_bow: 0.6987841247992659\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/17 11:41:25 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/07/17 11:41:26 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for column: sentence_bow_lem\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonydavid/Documents/Etudes/alternance_ML_engineer/OpenClassrooms/projets/projet_5/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score F1 (micro) for sentence_bow_lem: 0.3078838174273859\n",
      "Score F1 (weighted) for sentence_bow_lem: 0.2726941156918363\n",
      "Hamming Loss for sentence_bow_lem: 0.012755219086946547\n",
      "Jaccard Score for sentence_bow_lem: 0.06324080446585607\n",
      "Accuracy for sentence_bow_lem: 0.6960311998164717\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/17 11:41:38 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/07/17 11:41:40 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    }
   ],
   "source": [
    "# Entraîner et logguer les modèles pour chaque colonne\n",
    "columns_to_train = ['title', 'sentence_bow', 'sentence_bow_lem']\n",
    "\n",
    "for column in columns_to_train:\n",
    "    train_and_log_model(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder les DataFrames d'entraînement et de test dans des fichiers CSV\n",
    "df_train, df_test_with_tags = train_test_split(df_with_tags, test_size=0.2, random_state=42)\n",
    "df_test = pd.concat([df_test_with_tags, df_without_tags], ignore_index=True)\n",
    "df_train.to_csv('stackoverflow_train_tags_only.csv', index=False)\n",
    "df_test.to_csv('stackoverflow_test_tags_and_no_tags.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using splitted train/test v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_tags = 30\n",
    "\n",
    "# Créer une liste de tous les tags\n",
    "all_tags = [tag for tags in df_train['tags'].apply(eval) for tag in tags]  # Utiliser eval pour convertir les chaînes de listes en listes\n",
    "\n",
    "# Limiter les tags aux plus fréquents\n",
    "top_tags = [tag for tag, count in Counter(all_tags).most_common(number_of_tags)]\n",
    "\n",
    "# Filtrer les tags pour ne garder que les top \n",
    "df_train['filtered_tags'] = df_train['tags'].apply(lambda tags: [tag for tag in eval(tags) if tag in top_tags])\n",
    "df_test['filtered_tags'] = df_test['tags'].apply(lambda tags: [tag for tag in eval(tags) if tag in top_tags])\n",
    "\n",
    "# Supprimer les lignes sans tags pour df_train\n",
    "df_train = df_train[df_train['filtered_tags'].map(len) > 0]\n",
    "\n",
    "# Encoder les tags avec MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(classes=top_tags)\n",
    "y_train = mlb.fit_transform(df_train['filtered_tags'])\n",
    "y_test = mlb.transform(df_test['filtered_tags'])\n",
    "\n",
    "tfidf_max_features = 2000\n",
    "# Vectorisation TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=tfidf_max_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour entraîner, évaluer et logguer le modèle\n",
    "def train_and_log_model(column_name, svd_components=300):\n",
    "    print(f\"Training model for column: {column_name}\")\n",
    "\n",
    "    X_tfidf_train = tfidf_vectorizer.fit_transform(df_train[column_name])\n",
    "    X_tfidf_test = tfidf_vectorizer.transform(df_test[column_name])\n",
    "\n",
    "    # Réduction dimensionnelle avec SVD\n",
    "    svd = TruncatedSVD(n_components=svd_components)\n",
    "    X_train = svd.fit_transform(X_tfidf_train)\n",
    "    X_test = svd.transform(X_tfidf_test)\n",
    "\n",
    "    # Entraîner le modèle OneVsRestClassifier avec LogisticRegression\n",
    "    model = OneVsRestClassifier(LogisticRegression(max_iter=1000))\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Prédire sur l'ensemble de test\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculer et afficher les scores\n",
    "    f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "    hamming = hamming_loss(y_test, y_pred)\n",
    "    jaccard = jaccard_score(y_test, y_pred, average='samples')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Score F1 (micro) for {column_name}: {f1_micro}\")\n",
    "    print(f\"Score F1 (weighted) for {column_name}: {f1_weighted}\")\n",
    "    print(f\"Hamming Loss for {column_name}: {hamming}\")\n",
    "    print(f\"Jaccard Score for {column_name}: {jaccard}\")\n",
    "    print(f\"Accuracy for {column_name}: {accuracy}\")\n",
    "    print('-------------------------------------------------------------')\n",
    "\n",
    "    # Logger le modèle et les métriques sur MLflow\n",
    "    with mlflow.start_run(run_name=f\"{column_name}_model\") as run:\n",
    "    # Loguer les paramètres\n",
    "        mlflow.log_param(\"number_of_tags\", number_of_tags)\n",
    "        mlflow.log_param(\"max_features\", tfidf_max_features)\n",
    "        mlflow.log_param(\"n_components\", svd_components)\n",
    "        mlflow.log_param(\"test_size\", 0.2)\n",
    "        mlflow.log_param(\"model\", \"LogisticRegression - TF-IDF + SVD\")\n",
    "        \n",
    "        mlflow.log_param(\"column\", column_name)\n",
    "        mlflow.log_metric(\"f1_score_micro\", f1_micro)\n",
    "        mlflow.log_metric(\"f1_score_weighted\", f1_weighted)\n",
    "        mlflow.log_metric(\"hamming_loss\", hamming)\n",
    "        mlflow.log_metric(\"jaccard_score\", jaccard)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "        # Logger les modèles\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        mlflow.sklearn.log_model(tfidf_vectorizer, \"tfidf_vectorizer\")\n",
    "        mlflow.sklearn.log_model(svd, \"svd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for column: title\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonydavid/Documents/Etudes/alternance_ML_engineer/OpenClassrooms/projets/projet_5/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score F1 (micro) for title: 0.2991150442477876\n",
      "Score F1 (weighted) for title: 0.27419273149543383\n",
      "Hamming Loss for title: 0.02626865671641791\n",
      "Jaccard Score for title: 0.12723880597014925\n",
      "Accuracy for title: 0.38308457711442784\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/17 13:31:00 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/07/17 13:31:01 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for column: sentence_bow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonydavid/Documents/Etudes/alternance_ML_engineer/OpenClassrooms/projets/projet_5/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score F1 (micro) for sentence_bow: 0.28660994178235555\n",
      "Score F1 (weighted) for sentence_bow: 0.2612148145754262\n",
      "Hamming Loss for sentence_bow: 0.026417910447761195\n",
      "Jaccard Score for sentence_bow: 0.11977611940298508\n",
      "Accuracy for sentence_bow: 0.37860696517412934\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/17 13:31:07 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/07/17 13:31:08 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for column: sentence_bow_lem\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonydavid/Documents/Etudes/alternance_ML_engineer/OpenClassrooms/projets/projet_5/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score F1 (micro) for sentence_bow_lem: 0.29159212880143115\n",
      "Score F1 (weighted) for sentence_bow_lem: 0.2661763450249109\n",
      "Hamming Loss for sentence_bow_lem: 0.02626865671641791\n",
      "Jaccard Score for sentence_bow_lem: 0.12375621890547264\n",
      "Accuracy for sentence_bow_lem: 0.3850746268656716\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/17 13:31:15 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/07/17 13:31:16 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    }
   ],
   "source": [
    "# Entraîner et logguer les modèles pour chaque colonne\n",
    "columns_to_train = ['title', 'sentence_bow', 'sentence_bow_lem']\n",
    "\n",
    "for column in columns_to_train:\n",
    "    train_and_log_model(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
