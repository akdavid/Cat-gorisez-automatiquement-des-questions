{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, hamming_loss, jaccard_score, accuracy_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'title', 'tags', 'score', 'answer_count', 'sentence_bow',\n",
      "       'sentence_bow_lem', 'sentence_dl'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('stackoverflow_questions_cleaned_train.csv')\n",
    "df_test = pd.read_csv('stackoverflow_questions_cleaned_test.csv')\n",
    "\n",
    "# Afficher les colonnes disponibles\n",
    "print(df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_tags = 50\n",
    "\n",
    "# Créer une liste de tous les tags\n",
    "all_tags = [tag for tags in df_train['tags'].apply(eval) for tag in tags]  # Utiliser eval pour convertir les chaînes de listes en listes\n",
    "\n",
    "# Limiter les tags aux plus fréquents\n",
    "top_tags = [tag for tag, count in Counter(all_tags).most_common(number_of_tags)]\n",
    "\n",
    "# Filtrer les tags pour ne garder que les top \n",
    "df_train['filtered_tags'] = df_train['tags'].apply(lambda tags: [tag for tag in eval(tags) if tag in top_tags])\n",
    "df_test['filtered_tags'] = df_test['tags'].apply(lambda tags: [tag for tag in eval(tags) if tag in top_tags])\n",
    "\n",
    "# Supprimer les lignes sans tags pour df_train\n",
    "df_train = df_train[df_train['filtered_tags'].map(len) > 0]\n",
    "\n",
    "# Encoder les tags avec MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(classes=top_tags)\n",
    "y_train = mlb.fit_transform(df_train['filtered_tags'])\n",
    "y_test = mlb.transform(df_test['filtered_tags'])\n",
    "\n",
    "tfidf_max_features = 500\n",
    "# Vectorisation TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=tfidf_max_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour entraîner, évaluer et logguer le modèle\n",
    "def train_and_log_model(column_name, svd_components=300):\n",
    "    print(f\"Training model for column: {column_name}\")\n",
    "\n",
    "    X_tfidf_train = tfidf_vectorizer.fit_transform(df_train[column_name])\n",
    "    X_tfidf_test = tfidf_vectorizer.transform(df_test[column_name])\n",
    "\n",
    "    # Réduction dimensionnelle avec SVD\n",
    "    svd = TruncatedSVD(n_components=svd_components)\n",
    "    X_train = svd.fit_transform(X_tfidf_train)\n",
    "    X_test = svd.transform(X_tfidf_test)\n",
    "    \n",
    "    var_explained = svd.explained_variance_ratio_.sum()\n",
    "    print(f'var_explained = {var_explained}')\n",
    "\n",
    "    # Entraîner le modèle OneVsRestClassifier avec LogisticRegression\n",
    "    model = OneVsRestClassifier(LogisticRegression(max_iter=1000))\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Prédire sur l'ensemble de test\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculer et afficher les scores\n",
    "    f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "    hamming = hamming_loss(y_test, y_pred)\n",
    "    jaccard = jaccard_score(y_test, y_pred, average='samples')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Score F1 (micro) for {column_name}: {f1_micro}\")\n",
    "    print(f\"Score F1 (weighted) for {column_name}: {f1_weighted}\")\n",
    "    print(f\"Hamming Loss for {column_name}: {hamming}\")\n",
    "    print(f\"Jaccard Score for {column_name}: {jaccard}\")\n",
    "    print(f\"Accuracy for {column_name}: {accuracy}\")\n",
    "    print('-------------------------------------------------------------')\n",
    "\n",
    "    # Logger le modèle et les métriques sur MLflow\n",
    "    with mlflow.start_run(run_name=f\"{column_name}_model\") as run:\n",
    "    # Loguer les paramètres\n",
    "        mlflow.log_param(\"number_of_tags\", number_of_tags)\n",
    "        mlflow.log_param(\"max_features\", tfidf_max_features)\n",
    "        mlflow.log_param(\"n_components\", svd_components)\n",
    "        mlflow.log_param(\"var_explained\", var_explained)\n",
    "        mlflow.log_param(\"test_size\", 0.2)\n",
    "        mlflow.log_param(\"model\", \"LogisticRegression - TF-IDF + SVD\")\n",
    "        \n",
    "        mlflow.log_param(\"column\", column_name)\n",
    "        mlflow.log_metric(\"f1_score_micro\", f1_micro)\n",
    "        mlflow.log_metric(\"f1_score_weighted\", f1_weighted)\n",
    "        mlflow.log_metric(\"hamming_loss\", hamming)\n",
    "        mlflow.log_metric(\"jaccard_score\", jaccard)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "        # Logger les modèles\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        mlflow.sklearn.log_model(tfidf_vectorizer, \"tfidf_vectorizer\")\n",
    "        mlflow.sklearn.log_model(svd, \"svd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for column: title\n",
      "var_explained = 0.8236330557880823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonydavid/Documents/Etudes/alternance_ML_engineer/OpenClassrooms/projets/projet_5/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score F1 (micro) for title: 0.31049250535331907\n",
      "Score F1 (weighted) for title: 0.2890029192490611\n",
      "Hamming Loss for title: 0.019223880597014926\n",
      "Jaccard Score for title: 0.1472636815920398\n",
      "Accuracy for title: 0.31094527363184077\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/17 14:55:17 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/07/17 14:55:19 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for column: sentence_bow\n",
      "var_explained = 0.8091276245443009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonydavid/Documents/Etudes/alternance_ML_engineer/OpenClassrooms/projets/projet_5/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score F1 (micro) for sentence_bow: 0.312701252236136\n",
      "Score F1 (weighted) for sentence_bow: 0.2884291126844779\n",
      "Hamming Loss for sentence_bow: 0.019114427860696517\n",
      "Jaccard Score for sentence_bow: 0.1468905472636816\n",
      "Accuracy for sentence_bow: 0.3074626865671642\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/17 14:55:27 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/07/17 14:55:29 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for column: sentence_bow_lem\n",
      "var_explained = 0.8148789405804975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonydavid/Documents/Etudes/alternance_ML_engineer/OpenClassrooms/projets/projet_5/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score F1 (micro) for sentence_bow_lem: 0.3142857142857143\n",
      "Score F1 (weighted) for sentence_bow_lem: 0.29032875335314046\n",
      "Hamming Loss for sentence_bow_lem: 0.0191044776119403\n",
      "Jaccard Score for sentence_bow_lem: 0.14937810945273633\n",
      "Accuracy for sentence_bow_lem: 0.30845771144278605\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/17 14:55:41 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/07/17 14:55:43 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    }
   ],
   "source": [
    "# Entraîner et logguer les modèles pour chaque colonne\n",
    "columns_to_train = ['title', 'sentence_bow', 'sentence_bow_lem']\n",
    "\n",
    "for column in columns_to_train:\n",
    "    train_and_log_model(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
